## è¨­å®š OpenAI API Key è®Šæ•¸
from dotenv import load_dotenv
import os
import gradio as gr
import pymysql
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss
import jieba
from fuzzywuzzy import process
import numpy as np
from openai import OpenAI

# å–å¾—ç›®å‰æª”æ¡ˆçš„ç›®éŒ„å’Œå°ˆæ¡ˆæ ¹ç›®éŒ„
current_dir = Path(__file__).parent
root_dir = current_dir.parent

# è¼‰å…¥ .env æª”æ¡ˆï¼ˆä½¿ç”¨å®Œæ•´è·¯å¾‘ï¼‰
env_path = root_dir / '.env'
load_dotenv(env_path)

# è¨­å®š æ–°ç‰ˆ OpenAI API Key
openai_api_key = os.getenv('OPENAI_API_KEY')
if not openai_api_key:
    raise ValueError(f"âš ï¸ æ‰¾ä¸åˆ° OPENAI_API_KEYï¼Œè«‹ç¢ºèª .env æª”æ¡ˆå­˜åœ¨æ–¼ï¼š{env_path}")

client = OpenAI(api_key=openai_api_key)

# å®šç¾©å…¨åŸŸè®Šæ•¸å„²å­˜æ­·å²ç´€éŒ„
history = []

# ç³»çµ±æç¤ºè©
SYSTEM_PROMPT = """ä½ æ˜¯å°åŒ—å¸‚ç«‹åœ–æ›¸é¤¨çš„ AI å®¢æœï¼Œå°ˆé–€å›ç­”é—œæ–¼ã€Œåœ–æ›¸é¤¨ã€çš„å•é¡Œã€‚
æ“…é•·æ ¹æ“šè³‡æ–™åº«å…§å®¹æä¾›æ­£ç¢ºè³‡è¨Šä¸¦é€²è¡Œå‹å–„çš„äº’å‹•å›æ‡‰ã€‚
å¦‚æœç”¨æˆ¶çš„å•é¡Œè¶…å‡ºé€™äº›ç¯„åœï¼Œè«‹å›è¦†ï¼š
ã€æŠ±æ­‰ï¼Œæˆ‘åƒ…é™æ–¼ã€Œåœ–æ›¸é¤¨ã€ç›¸é—œè©±é¡Œå–”ï¼æ‚¨å¯ä»¥å˜—è©¦åœ¨ç¶²è·¯ä¸Šæœå°‹ç›¸é—œè³‡è¨Šã€‚ã€

ä½ çš„ä»»å‹™æ˜¯ï¼š
* æ ¹æ“šç”¨æˆ¶å•é¡Œï¼Œå¾è³‡æ–™åº«ä¸­æª¢ç´¢å‡ºæœ€ç›¸é—œçš„å…§å®¹
* å„ªå…ˆåƒè€ƒè³‡æ–™åº«æª¢ç´¢çµæœé€²è¡Œå›ç­”ï¼Œä¸¦åœ¨å›ç­”ä¸­é«”ç¾æª¢ç´¢å…§å®¹
* å¦‚æœè³‡æ–™åº«æ²’æœ‰ç›´æ¥ç›¸é—œç­”æ¡ˆï¼Œæ ¹æ“šèªç¾©ç›¸ä¼¼åº¦çµæœæä¾›å»¶ä¼¸å»ºè­°æˆ–å¯èƒ½çš„è³‡è¨Š
* çµåˆå¤šç­†è³‡æ–™åº«çµæœï¼Œæ•´ç†å‡ºå®Œæ•´ã€ç°¡æ˜ä¸”å‹å–„çš„ç­”æ¡ˆ

å›ç­”æ™‚è«‹éµå¾ªï¼š
- ä½¿ç”¨æ­£é«”ä¸­æ–‡ï¼Œèªæ°£å°ˆæ¥­ä¸”è¦ªåˆ‡
- å…ˆæä¾›ç›´æ¥ç­”æ¡ˆï¼Œå†è£œå……èªªæ˜ï¼ˆå¦‚æœ‰å»¶ä¼¸å»ºè­°ï¼‰
- ç¢ºä¿ä¿¡æ¯æº–ç¢ºæ€§ï¼Œé¿å…çŒœæ¸¬
- åœ¨è³‡æ–™åº«æª¢ç´¢çµæœä¸­ï¼Œæ¨™è¨»è³‡æ–™ä¾†æºæˆ–é—œéµè³‡è¨Š
- è‹¥ç„¡ç›¸é—œçµæœï¼Œèªªæ˜è³‡æ–™åº«æŸ¥è©¢ç„¡çµæœä¸¦æä¾›å…¶ä»–å»ºè­°

ç¯„ä¾‹å›ç­”æ ¼å¼ï¼š
1. **ç­”æ¡ˆï¼š** æä¾›è³‡æ–™åº«ä¸­æœ€ç›¸é—œçš„ç­”æ¡ˆ
2. **è£œå……èªªæ˜ï¼š** æ ¹æ“šè³‡æ–™åº«å…§å®¹å»¶ä¼¸çš„èƒŒæ™¯è³‡è¨Š"""

desc = "ç›´æ¥è¼¸å…¥æ‚¨çš„å•é¡Œæˆ–é—œéµè© " \
       "AI åŠ©æ‰‹æœƒæ ¹æ“šåœ–æ›¸é¤¨å•ç­”è³‡æ–™åº«å›å¾©ç›¸é—œæœå‹™èˆ‡æ”¿ç­–ã€‚" 

article = "<h1>åœ–æ›¸é¤¨å®¢æœå•ç­”ç³»çµ± </h1>"\
          "<h3>ä½¿ç”¨èªªæ˜:</h3> " \
          "<ul><li>ç›´æ¥è¼¸å…¥æ‚¨çš„å•é¡Œæˆ–é—œéµè©</li>" \
          "<li>AI åŠ©æ‰‹æœƒæ ¹æ“šåœ–æ›¸é¤¨å•ç­”è³‡æ–™åº«å›å¾©ç›¸é—œæœå‹™èˆ‡æ”¿ç­–</li></ul>"

class QASystem:
    index_cache = None  # é¡åˆ¥éœæ…‹å±¬æ€§ (å„²å­˜ç´¢å¼•)

    def __init__(self):
        # åˆå§‹åŒ–æ¨¡å‹
        self.model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        self.model = SentenceTransformer(self.model_name)
        self.index_path = root_dir / 'Vector_index' / 'vector_model4.index'
        
        if QASystem.index_cache is None:
            print(f"é¦–æ¬¡è¼‰å…¥ç´¢å¼•ï¼š{self.index_path}")
            try:
                QASystem.index_cache = faiss.read_index(str(self.index_path))
                print("ç´¢å¼•è¼‰å…¥æˆåŠŸï¼")
            except Exception as e:
                print(f"è®€å–ç´¢å¼•æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                raise

        # è®“å¯¦ä¾‹å…±ç”¨éœæ…‹ç´¢å¼•
        self.index = QASystem.index_cache
        
        # è³‡æ–™åº«é€£ç·š
        try:
            self.connection = connect_db()
        except Exception as e:
            print(f"è³‡æ–™åº«é€£ç·šéŒ¯èª¤ï¼š{e}")
            raise

        # å¾è³‡æ–™åº«è®€å–å•ç­”è³‡æ–™
        db_results = fetch_data_from_db(self.connection)
        if not db_results:
            raise ValueError("è³‡æ–™åº«ä¸­æ²’æœ‰å•ç­”è³‡æ–™ï¼")
        
        # è™•ç†å•ç­”è³‡æ–™  ä»¥å•é¡Œç‚ºéµï¼Œç­”æ¡ˆç‚ºå€¼
        self.questions = [item['question'] for item in db_results]
        self.answers = {item['question']: item['answer'] for item in db_results}

    def search_similar_questions(self, query, top_k=3):
        # 1. Jieba æ–·è©
        query_tokens = " ".join(jieba.cut(query))
        print(f"æ–·è©: {query_tokens}") 
        
        # æ¨¡ç³Šæœå°‹ (FuzzyWuzzy)
        fuzzy_results = process.extract(query_tokens, self.questions, limit=10, scorer=process.fuzz.partial_ratio)
        fuzzy_candidates = {res[0]: res[1] / 100 for res in fuzzy_results if res[1] > 60}

        # èªç¾©æª¢ç´¢ (FAISS)
        query_embedding = self.model.encode([query], normalize_embeddings=True)
        _, top_indices = self.index.search(np.array(query_embedding).astype('float32'), top_k)

        faiss_candidates = {self.questions[idx]: 1.0 for idx in top_indices[0] if idx < len(self.questions)}

        # åˆä½µçµæœï¼Œä¸¦åŠ æ¬Š
        merged_scores = {}
        for q in set(fuzzy_candidates.keys()).union(faiss_candidates.keys()):
            fuzzy_score = fuzzy_candidates.get(q, 0)
            faiss_score = faiss_candidates.get(q, 0)
            merged_scores[q] = 0.6 * fuzzy_score + 0.4 * faiss_score

        # æ’åºä¸¦å›å‚³
        sorted_results = sorted(merged_scores.items(), key=lambda x: x[1], reverse=True)
        final = [(q, self.answers[q]) for q, _ in sorted_results if q in self.answers]
        return final[:top_k]
    
    def __del__(self):
        if self.connection and self.connection.open:  # ç¢ºä¿é€£ç·šå­˜åœ¨ä¸”æœªé—œé–‰
            self.connection.close()
            print("ğŸ”Œ è³‡æ–™åº«é€£ç·šå·²é—œé–‰")
        

def handle_user_query(user_query):
    try:
        qa_system = QASystem()
        db_results = qa_system.search_similar_questions(user_query)  # ç¢ºä¿é€™è£¡èƒ½å¤ ç²å–åˆ°çµæœ
        print(f"æŸ¥è©¢çµæœï¼š{db_results}")  # æ·»åŠ æ—¥èªŒè¼¸å‡º
        return db_results
    except Exception as e:
        print(f"è™•ç†æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æ‹‹å‡ºéŒ¯èª¤

# é€£æ¥åˆ° MySQL è³‡æ–™åº«
def connect_db():
    try:
        conn = pymysql.connect(
            host='localhost',
            user=os.getenv('DB_USER'),
            password=os.getenv('DB_PASSWORD'),
            database=os.getenv('DB_NAME'),
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
        print("è³‡æ–™åº«é€£æ¥æˆåŠŸï¼")  # æ·»åŠ æ—¥èªŒè¼¸å‡º
        return conn
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£ç·šåˆ°è³‡æ–™åº«: {e}")
        return None

# 2. å¾è³‡æ–™åº«è®€å– FAQ
def fetch_data_from_db(conn):
    try:
        with conn.cursor() as cursor:
            sql = "SELECT id, question, answer FROM library_qa"
            cursor.execute(sql)
            results = cursor.fetchall()
            # print(f"è®€å–åˆ°çš„è³‡æ–™ï¼š{results}")  # æ·»åŠ æ—¥èªŒè¼¸å‡º
            return results
    except Exception as e:
        print(f"è³‡æ–™åº«è®€å–å¤±æ•—: {e}")
        return []

def get_ai_response(user_query, db_results):
    global history  # ä½¿ç”¨å…¨åŸŸè®Šæ•¸
    try:
        if db_results is None or len(db_results) == 0:  # æª¢æŸ¥ db_results æ˜¯å¦ç‚ºç©º
            return "è³‡æ–™åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ç­”æ¡ˆã€‚è«‹æä¾›ä¸€å€‹åˆé©çš„å›æ‡‰ã€‚"

        context = f"ç”¨æˆ¶å•é¡Œï¼š{user_query}\n\n"
        context += "ç›¸é—œçš„è³‡æ–™åº«çµæœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰ï¼š\n"
        
        for question, answer in db_results:  # ç¢ºä¿ db_results æ˜¯æ­£ç¢ºçš„æ ¼å¼
            context += f"[ç›¸ä¼¼åº¦: {0:.4f}]\n"  # å‡è¨­ç›¸ä¼¼åº¦ç‚º 0ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´
            context += f"å•ï¼š{question}\n"
            context += f"ç­”ï¼š{answer}\n\n"

        # # é™åˆ¶æ­·å²ç´€éŒ„åƒ…ä¿ç•™æœ€è¿‘ 10 ç­†å°è©±
        # if len(history) > 10:
        #     history.pop(0)

        # å°‡ç”¨æˆ¶è¼¸å…¥åŠ å…¥æ­·å²ç´€éŒ„
        history.append({"role": "user", "content": user_query})

        # æ•´åˆæ­·å²å°è©± + ç³»çµ±æç¤º + è³‡æ–™åº«çµæœ
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT}] + history + [{"role": "system", "content": context}]
        
        # å‘¼å« OpenAI API
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", 
            messages=messages,
            temperature=0.7,
            stream=True,
        )

        # å„²å­˜ AI å›æ‡‰
        full_response = ""
        for chunk in response:
            content = getattr(chunk.choices[0].delta, 'content', '')
            if content:
                full_response += content
        history.append({"role": "assistant", "content": full_response.strip()})
        history = history[-10:]  # ä¿ç•™æœ€å¾Œ 10 ç­†è³‡æ–™

        print(f"ç›®å‰çš„æ­·å²ç´€éŒ„ï¼š{history}")
        print(f"ç›®å‰ç¸½å…±å„²å­˜çš„å°è©±æ•¸ï¼š{len(history)}")

        return full_response.strip() if full_response else "AI å›æ‡‰ç”Ÿæˆå¤±æ•—ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚"

    except Exception as e:
        print(f"AI è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        import traceback
        print(f"è©³ç´°éŒ¯èª¤ï¼š\n{traceback.format_exc()}")
        return f"å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•æä¾›é©ç•¶çš„å›æ‡‰ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"

def chat_response(user_query, history=None):
    history = history or []
    try:
        # å…ˆæŸ¥è©¢è³‡æ–™åº«
        db_results = handle_user_query(user_query)
        
        # å†ä½¿ç”¨ AI ç”Ÿæˆå›æ‡‰
        response = get_ai_response(user_query, db_results)
        print(response)

        return response
    except Exception as e:
        print(f"èŠå¤©å›æ‡‰ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return "æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

# ä¿®æ”¹ ChatInterface çš„è¨­ç½®
gr.close_all()
gr.ChatInterface(
    fn=chat_response,  # æ”¹ç”¨ chat_response è€Œä¸æ˜¯ get_ai_response
    theme="Soft",
    title=article,
    examples=[
        "è«‹å•åœ–æ›¸é¤¨çš„é–‹æ”¾æ™‚é–“æ˜¯ä»€éº¼æ™‚å€™ï¼Ÿ",
        "æˆ‘æƒ³äº†è§£å¦‚ä½•è¾¦ç†å€Ÿæ›¸è­‰",
        "å…’ç«¥å€æœ‰ä»€éº¼ç‰¹åˆ¥çš„æœå‹™ï¼Ÿ",
        "å¹«æˆ‘ä»‹ç´¹ä¸€ä¸‹åœ–æ›¸é¤¨çš„è³‡æºå—ï¼Ÿ"
    ]
).queue().launch(
    debug=True
)